{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c0660d54-8cc8-4157-a3bf-ab002e60cebb",
   "metadata": {
    "editable": true,
    "scrolled": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02412bb697174030aa3b712891139d4c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(button_style='success', description='Record', icon='microphone', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce37e6015eca40e8b10c003c9e39160b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(button_style='warning', description='Stop', icon='stop', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba06c5f100b74746b3ee5322238fa2e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "from threading import Thread\n",
    "from queue import Queue # well let us pass messages between threads\n",
    "\n",
    "messages = Queue() # will tell thread when to stop recording and transcribing\n",
    "recordings = Queue() # will store audio from mic and pass it to transcription\n",
    "TRANSCRIPT_FILENAME = \"transcript_files/fulltranscript.txt\"\n",
    "# delete contents of transcription file\n",
    "f = open(TRANSCRIPT_FILENAME, \"w\") \n",
    "f.write('')\n",
    "f.close()\n",
    "\n",
    "record_button = widgets.Button(\n",
    "    description=\"Record\",\n",
    "    disabled=False,\n",
    "    button_style=\"success\",\n",
    "    icon=\"microphone\"\n",
    ")\n",
    "\n",
    "stop_button = widgets.Button(\n",
    "    description=\"Stop\",\n",
    "    disabled=False,\n",
    "    button_style=\"warning\",\n",
    "    icon=\"stop\"\n",
    ")\n",
    "\n",
    "# output widget will show transcript as it's generated\n",
    "output = widgets.Output()\n",
    "\n",
    "# data is automatically passed by jupyter notebook\n",
    "def start_recording(data):\n",
    "    print('recording')\n",
    "    messages.put(True) # will tell thread to keep running and recording\n",
    "\n",
    "    with output: \n",
    "        display(\"Starting...\")\n",
    "\n",
    "        # thread that will record microphone\n",
    "        record = Thread(target=record_microphone) \n",
    "        record.start()\n",
    "\n",
    "        # thread that will start transcribing\n",
    "        transcribe = Thread(target=speech_recognition, args=(output,))\n",
    "        transcribe.start()\n",
    "\n",
    "        # thread that will answer questions\n",
    "        # chatbot_thread = Thread(target=chatbot)\n",
    "        # chatbot_thread.start()\n",
    "\n",
    "def stop_recording(data):\n",
    "    # with output: \n",
    "        messages.get() # will take the 'True' message off the queue\n",
    "        display(\"Stopped.\")\n",
    "\n",
    "record_button.on_click(start_recording)\n",
    "stop_button.on_click(stop_recording)\n",
    "    \n",
    "\n",
    "display(record_button, stop_button, output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a595e1d8-10ab-4e99-841a-5e9000bafd26",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert/distilbert-base-cased-distilled-squad and revision 626af31 (https://huggingface.co/distilbert/distilbert-base-cased-distilled-squad).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "qa_pipeline = pipeline(\"question-answering\")\n",
    "\n",
    "def chatbot():\n",
    "    while True: \n",
    "        transcription = open(TRANSCRIPT_FILENAME, \"r\").read()\n",
    "        question = input('You: ')\n",
    "        result = qa_pipeline(question=question, context=transcription)\n",
    "        print(result['answer'])\n",
    "        \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "df30871f-7653-4095-a87d-4f3bf6338632",
   "metadata": {
    "editable": true,
    "scrolled": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'index': 0, 'structVersion': 2, 'name': 'HDA Intel PCH: ALC289 Analog (hw:0,0)', 'hostApi': 0, 'maxInputChannels': 2, 'maxOutputChannels': 2, 'defaultLowInputLatency': 0.005804988662131519, 'defaultLowOutputLatency': 0.005333333333333333, 'defaultHighInputLatency': 0.034829931972789115, 'defaultHighOutputLatency': 0.032, 'defaultSampleRate': 48000.0}\n",
      "{'index': 1, 'structVersion': 2, 'name': 'HDA Intel PCH: HDMI 0 (hw:0,3)', 'hostApi': 0, 'maxInputChannels': 0, 'maxOutputChannels': 8, 'defaultLowInputLatency': -1.0, 'defaultLowOutputLatency': 0.005804988662131519, 'defaultHighInputLatency': -1.0, 'defaultHighOutputLatency': 0.034829931972789115, 'defaultSampleRate': 44100.0}\n",
      "{'index': 2, 'structVersion': 2, 'name': 'HDA Intel PCH: HDMI 1 (hw:0,7)', 'hostApi': 0, 'maxInputChannels': 0, 'maxOutputChannels': 8, 'defaultLowInputLatency': -1.0, 'defaultLowOutputLatency': 0.005804988662131519, 'defaultHighInputLatency': -1.0, 'defaultHighOutputLatency': 0.034829931972789115, 'defaultSampleRate': 44100.0}\n",
      "{'index': 3, 'structVersion': 2, 'name': 'HDA Intel PCH: HDMI 2 (hw:0,8)', 'hostApi': 0, 'maxInputChannels': 0, 'maxOutputChannels': 8, 'defaultLowInputLatency': -1.0, 'defaultLowOutputLatency': 0.005804988662131519, 'defaultHighInputLatency': -1.0, 'defaultHighOutputLatency': 0.034829931972789115, 'defaultSampleRate': 44100.0}\n",
      "{'index': 4, 'structVersion': 2, 'name': 'HDA Intel PCH: HDMI 3 (hw:0,9)', 'hostApi': 0, 'maxInputChannels': 0, 'maxOutputChannels': 8, 'defaultLowInputLatency': -1.0, 'defaultLowOutputLatency': 0.005804988662131519, 'defaultHighInputLatency': -1.0, 'defaultHighOutputLatency': 0.034829931972789115, 'defaultSampleRate': 44100.0}\n",
      "{'index': 5, 'structVersion': 2, 'name': 'Dell Headset WH3022: USB Audio (hw:1,0)', 'hostApi': 0, 'maxInputChannels': 1, 'maxOutputChannels': 2, 'defaultLowInputLatency': 0.0239375, 'defaultLowOutputLatency': 0.0239375, 'defaultHighInputLatency': 0.096, 'defaultHighOutputLatency': 0.096, 'defaultSampleRate': 16000.0}\n",
      "{'index': 6, 'structVersion': 2, 'name': 'sysdefault', 'hostApi': 0, 'maxInputChannels': 128, 'maxOutputChannels': 128, 'defaultLowInputLatency': 0.021333333333333333, 'defaultLowOutputLatency': 0.021333333333333333, 'defaultHighInputLatency': 0.021333333333333333, 'defaultHighOutputLatency': 0.021333333333333333, 'defaultSampleRate': 48000.0}\n",
      "{'index': 7, 'structVersion': 2, 'name': 'front', 'hostApi': 0, 'maxInputChannels': 0, 'maxOutputChannels': 2, 'defaultLowInputLatency': -1.0, 'defaultLowOutputLatency': 0.005333333333333333, 'defaultHighInputLatency': -1.0, 'defaultHighOutputLatency': 0.032, 'defaultSampleRate': 48000.0}\n",
      "{'index': 8, 'structVersion': 2, 'name': 'surround40', 'hostApi': 0, 'maxInputChannels': 0, 'maxOutputChannels': 2, 'defaultLowInputLatency': -1.0, 'defaultLowOutputLatency': 0.005333333333333333, 'defaultHighInputLatency': -1.0, 'defaultHighOutputLatency': 0.032, 'defaultSampleRate': 48000.0}\n",
      "{'index': 9, 'structVersion': 2, 'name': 'surround51', 'hostApi': 0, 'maxInputChannels': 0, 'maxOutputChannels': 2, 'defaultLowInputLatency': -1.0, 'defaultLowOutputLatency': 0.005333333333333333, 'defaultHighInputLatency': -1.0, 'defaultHighOutputLatency': 0.032, 'defaultSampleRate': 48000.0}\n",
      "{'index': 10, 'structVersion': 2, 'name': 'surround71', 'hostApi': 0, 'maxInputChannels': 0, 'maxOutputChannels': 2, 'defaultLowInputLatency': -1.0, 'defaultLowOutputLatency': 0.005333333333333333, 'defaultHighInputLatency': -1.0, 'defaultHighOutputLatency': 0.032, 'defaultSampleRate': 48000.0}\n",
      "{'index': 11, 'structVersion': 2, 'name': 'hdmi', 'hostApi': 0, 'maxInputChannels': 0, 'maxOutputChannels': 8, 'defaultLowInputLatency': -1.0, 'defaultLowOutputLatency': 0.005804988662131519, 'defaultHighInputLatency': -1.0, 'defaultHighOutputLatency': 0.034829931972789115, 'defaultSampleRate': 44100.0}\n",
      "{'index': 12, 'structVersion': 2, 'name': 'samplerate', 'hostApi': 0, 'maxInputChannels': 128, 'maxOutputChannels': 128, 'defaultLowInputLatency': 0.007755102040816327, 'defaultLowOutputLatency': 0.007755102040816327, 'defaultHighInputLatency': 0.023219954648526078, 'defaultHighOutputLatency': 0.023219954648526078, 'defaultSampleRate': 44100.0}\n",
      "{'index': 13, 'structVersion': 2, 'name': 'speexrate', 'hostApi': 0, 'maxInputChannels': 128, 'maxOutputChannels': 128, 'defaultLowInputLatency': 0.007755102040816327, 'defaultLowOutputLatency': 0.007755102040816327, 'defaultHighInputLatency': 0.023219954648526078, 'defaultHighOutputLatency': 0.023219954648526078, 'defaultSampleRate': 44100.0}\n",
      "{'index': 14, 'structVersion': 2, 'name': 'pulse', 'hostApi': 0, 'maxInputChannels': 32, 'maxOutputChannels': 32, 'defaultLowInputLatency': 0.008684807256235827, 'defaultLowOutputLatency': 0.008684807256235827, 'defaultHighInputLatency': 0.034807256235827665, 'defaultHighOutputLatency': 0.034807256235827665, 'defaultSampleRate': 44100.0}\n",
      "{'index': 15, 'structVersion': 2, 'name': 'upmix', 'hostApi': 0, 'maxInputChannels': 8, 'maxOutputChannels': 8, 'defaultLowInputLatency': 0.005804988662131519, 'defaultLowOutputLatency': 0.007755102040816327, 'defaultHighInputLatency': 0.034829931972789115, 'defaultHighOutputLatency': 0.023219954648526078, 'defaultSampleRate': 44100.0}\n",
      "{'index': 16, 'structVersion': 2, 'name': 'vdownmix', 'hostApi': 0, 'maxInputChannels': 6, 'maxOutputChannels': 6, 'defaultLowInputLatency': 0.005804988662131519, 'defaultLowOutputLatency': 0.007755102040816327, 'defaultHighInputLatency': 0.034829931972789115, 'defaultHighOutputLatency': 0.023219954648526078, 'defaultSampleRate': 44100.0}\n",
      "{'index': 17, 'structVersion': 2, 'name': 'dmix', 'hostApi': 0, 'maxInputChannels': 0, 'maxOutputChannels': 2, 'defaultLowInputLatency': -1.0, 'defaultLowOutputLatency': 0.021333333333333333, 'defaultHighInputLatency': -1.0, 'defaultHighOutputLatency': 0.021333333333333333, 'defaultSampleRate': 48000.0}\n",
      "{'index': 18, 'structVersion': 2, 'name': 'default', 'hostApi': 0, 'maxInputChannels': 32, 'maxOutputChannels': 32, 'defaultLowInputLatency': 0.008684807256235827, 'defaultLowOutputLatency': 0.008684807256235827, 'defaultHighInputLatency': 0.034807256235827665, 'defaultHighOutputLatency': 0.034807256235827665, 'defaultSampleRate': 44100.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ALSA lib pcm.c:2664:(snd_pcm_open_noupdate) Unknown PCM cards.pcm.rear\n",
      "ALSA lib pcm.c:2664:(snd_pcm_open_noupdate) Unknown PCM cards.pcm.center_lfe\n",
      "ALSA lib pcm.c:2664:(snd_pcm_open_noupdate) Unknown PCM cards.pcm.side\n",
      "ALSA lib pcm_route.c:877:(find_matching_chmap) Found no matching channel map\n",
      "ALSA lib pcm_route.c:877:(find_matching_chmap) Found no matching channel map\n",
      "ALSA lib pcm_route.c:877:(find_matching_chmap) Found no matching channel map\n",
      "ALSA lib pcm_route.c:877:(find_matching_chmap) Found no matching channel map\n",
      "ALSA lib pcm_oss.c:397:(_snd_pcm_oss_open) Cannot open device /dev/dsp\n",
      "ALSA lib pcm_oss.c:397:(_snd_pcm_oss_open) Cannot open device /dev/dsp\n",
      "ALSA lib confmisc.c:160:(snd_config_get_card) Invalid field card\n",
      "ALSA lib pcm_usb_stream.c:482:(_snd_pcm_usb_stream_open) Invalid card 'card'\n",
      "ALSA lib confmisc.c:160:(snd_config_get_card) Invalid field card\n",
      "ALSA lib pcm_usb_stream.c:482:(_snd_pcm_usb_stream_open) Invalid card 'card'\n"
     ]
    }
   ],
   "source": [
    "# Figure out which microphone we want to use \n",
    "import pyaudio\n",
    "\n",
    "p = pyaudio.PyAudio()\n",
    "for i in range(p.get_device_count()):\n",
    "    print(p.get_device_info_by_index(i))\n",
    "\n",
    "p.terminate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87f270d7-c73d-4fed-b922-ed85a1b6946a",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "##### The microphone I want is: \n",
    "``` {'index': 5, 'structVersion': 2, 'name': 'Dell Headset WH3022: USB Audio (hw:1,0)', 'hostApi': 0, 'maxInputChannels': 1, 'maxOutputChannels': 2, 'defaultLowInputLatency': 0.0239375, 'defaultLowOutputLatency': 0.0239375, 'defaultHighInputLatency': 0.096, 'defaultHighOutputLatency': 0.096, 'defaultSampleRate': 16000.0} ```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3c7a14e4-5852-4b20-89f0-665842f1a060",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# constants that will allow for optimal conditions for speech recognition\n",
    "CHANNELS = 1\n",
    "FRAME_RATE = 16000 # Determines how high quality the frame rate is. how quickly the audio signal is sampled. unit is kHz = cycles/sec?\n",
    "RECORD_SECONDS = 3 # How many seconds we want to record audio for before we send it off for transcription. every 20s we'll generate a transcript\n",
    "AUDIO_FORMAT = pyaudio.paInt16\n",
    "SAMPLE_SIZE = 2\n",
    "\n",
    "\n",
    "def record_microphone(chunk=1024): # chunk is how often we are going to read audio from the microphone (how many audio frames).\n",
    "    p = pyaudio.PyAudio()\n",
    "\n",
    "    stream = p.open(format=AUDIO_FORMAT,\n",
    "                    channels=CHANNELS,\n",
    "                    rate=FRAME_RATE,\n",
    "                    input=True,\n",
    "                    # input_device_index=5,\n",
    "                    frames_per_buffer=chunk)\n",
    "                    \n",
    "    frames = [] # will store all the audio recorded from the microphone\n",
    "\n",
    "    while not messages.empty():\n",
    "        data = stream.read(chunk)\n",
    "        frames.append(data)\n",
    "\n",
    "        # if we recorded more than 20 seconds of audio, then add audio data to recordings queue\n",
    "        if len(frames) >= (FRAME_RATE * RECORD_SECONDS) / chunk: \n",
    "            recordings.put(frames.copy())\n",
    "            frames = []\n",
    "\n",
    "    stream.stop_stream()\n",
    "    stream.close()\n",
    "    p.terminate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aff92f0e-d39e-448a-9111-c44b14c7f957",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%pip install vosk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79cf05df-0f3a-44aa-9371-4cebfd5be64e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%pip install transformers # needed for recase punc (to add punctuation back to transcript)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc2191ab-6f18-4afd-bf5d-3ee7be499599",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%pip install torch # requirement for recase punc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c971c9af-3442-46e0-a0f2-f3a131f3c94b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOG (VoskAPI:ReadDataFiles():model.cc:213) Decoding params beam=13 max-active=7000 lattice-beam=6\n",
      "LOG (VoskAPI:ReadDataFiles():model.cc:216) Silence phones 1:2:3:4:5:11:12:13:14:15\n",
      "LOG (VoskAPI:RemoveOrphanNodes():nnet-nnet.cc:948) Removed 0 orphan nodes.\n",
      "LOG (VoskAPI:RemoveOrphanComponents():nnet-nnet.cc:847) Removing 0 orphan components.\n",
      "LOG (VoskAPI:ReadDataFiles():model.cc:248) Loading i-vector extractor from /home/mariam/.cache/vosk/vosk-model-en-us-0.22/ivector/final.ie\n",
      "LOG (VoskAPI:ComputeDerivedVars():ivector-extractor.cc:183) Computing derived variables for iVector extractor\n",
      "LOG (VoskAPI:ComputeDerivedVars():ivector-extractor.cc:204) Done.\n",
      "LOG (VoskAPI:ReadDataFiles():model.cc:279) Loading HCLG from /home/mariam/.cache/vosk/vosk-model-en-us-0.22/graph/HCLG.fst\n",
      "LOG (VoskAPI:ReadDataFiles():model.cc:297) Loading words from /home/mariam/.cache/vosk/vosk-model-en-us-0.22/graph/words.txt\n",
      "LOG (VoskAPI:ReadDataFiles():model.cc:308) Loading winfo /home/mariam/.cache/vosk/vosk-model-en-us-0.22/graph/phones/word_boundary.int\n",
      "LOG (VoskAPI:ReadDataFiles():model.cc:315) Loading subtract G.fst model from /home/mariam/.cache/vosk/vosk-model-en-us-0.22/rescore/G.fst\n",
      "LOG (VoskAPI:ReadDataFiles():model.cc:317) Loading CARPA model from /home/mariam/.cache/vosk/vosk-model-en-us-0.22/rescore/G.carpa\n",
      "LOG (VoskAPI:ReadDataFiles():model.cc:323) Loading RNNLM model from /home/mariam/.cache/vosk/vosk-model-en-us-0.22/rnnlm/final.raw\n"
     ]
    }
   ],
   "source": [
    "import subprocess # will be used to call the punctuation model\n",
    "import json\n",
    "from vosk import Model, KaldiRecognizer\n",
    "\n",
    "model = Model(model_name=\"vosk-model-en-us-0.22\")\n",
    "rec = KaldiRecognizer(model, FRAME_RATE) # responsible for managing the audio transcription \n",
    "rec.SetWords(True) # will give us confidence levels for each individual word\n",
    "\n",
    "# takes in the output widget so that it can display the transcript live.\n",
    "def speech_recognition(output):\n",
    "    global text_so_far\n",
    "    \n",
    "    while not messages.empty():\n",
    "        frames = recordings.get()\n",
    "        raw_audio_data = b''.join(frames)\n",
    "\n",
    "        rec.AcceptWaveform(b''.join(frames)) # join all chunks together into one binary string\n",
    "        result = rec.Result()\n",
    "        text = json.loads(result)[\"text\"]\n",
    "\n",
    "        # write to file\n",
    "        f = open(TRANSCRIPT_FILENAME,\"a\")\n",
    "        f.write(text)\n",
    "        f.close()\n",
    "        # now we have to add punctuation to our transcript\n",
    "        # this method is inefficient, because we're reloading the model and re-initializing everything every time \n",
    "        # instead,if you don't use the command line, it would be a lot faster and you can reduce the RECORD_SECONDS to 2-3 seconds which will make the transcription a lot more live. \n",
    "        # cased = subprocess.check_output(\"python recasepunc/recasepunc.py predict recasepunc/checkpoint/path\", shell=True,text=True,input=text)\n",
    "        output.append_stdout(text) # add transcript to output widget\n",
    "\n",
    "   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9f62b5c9-c378-48df-869d-32f87fc2ee33",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from pydub import AudioSegment\n",
    "# import whisper\n",
    "# import json\n",
    "# import io\n",
    "\n",
    "# model = whisper.load_model(\"base\")\n",
    "\n",
    "# # takes in the output widget so that it can display the transcript live.\n",
    "# def speech_recognition_whisper(output):\n",
    "#     while not messages.empty():\n",
    "#         frames = recordings.get()\n",
    "#         raw_audio_data = b''.join(frames)\n",
    "\n",
    "#         sound = AudioSegment(\n",
    "#             data=raw_audio_data,\n",
    "#             sample_width=SAMPLE_SIZE, # 2 byte (16 bit) samples\n",
    "#             frame_rate=FRAME_RATE,\n",
    "#             channels=2 # stereo\n",
    "#         )\n",
    "\n",
    "#         sound_file = sound.export('file.mp3',format='mp3')\n",
    "#         transcript = model.transcribe(sound_file)\n",
    "\n",
    "#         output.append_stdout(transcript['text']) # add transcript to output widget\n",
    "\n",
    "#         # # OR\n",
    "#         # buffer = io.BytesIO(raw_audio_data)\n",
    "#         # buffer.name = 'temp-file.mp3'\n",
    "\n",
    "#         # transcript = model.transcribe(buffer)\n",
    "\n",
    "#         # output.append_stdout(transcript['text'])\n",
    "\n",
    "    \n",
    "# # from faster_whisper import WhisperModel\n",
    "# # print(whisper.__file__)\n",
    "# # model = WhisperModel(model_size, device=\"cpu\", compute_type=\"int8\")\n",
    "\n",
    "# # def speech_recognition(output):\n",
    "# #     while not messages.empty():\n",
    "# #         frames = recordings.get()\n",
    "\n",
    "# #         result = model.transcribe(b''.join(frames))\n",
    "# #         text = result[\"text\"]\n",
    "# #         print(text)\n",
    "# #         output.append_stdout(text)\n",
    "       "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
